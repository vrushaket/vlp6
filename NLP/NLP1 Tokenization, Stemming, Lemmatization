{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWo53D2+ORN16PUvsUI2Gr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK\n","library. \n","\n","Use porter stemmer and snowball stemmer for stemming. \n","\n","Use any technique for lemmatization.\n","\n","Input / Dataset â€“use any sample sentence"],"metadata":{"id":"K492C1FsDyUz"}},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zknvSETDeY-","executionInfo":{"status":"ok","timestamp":1683108572014,"user_tz":-330,"elapsed":4732,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"7137429e-c72a-4e5d-bc64-0f9b3cdcc370"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoI9qvm0D9zA","executionInfo":{"status":"ok","timestamp":1683108572015,"user_tz":-330,"elapsed":14,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"5f99eb1f-43bd-4987-fddb-0cefb229bc17"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["##Tokenization"],"metadata":{"id":"JFHQ70B5F2Q4"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize,TreebankWordTokenizer,TweetTokenizer,MWETokenizer"],"metadata":{"id":"Kzd-JOSAGDvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Whitespace Tokenization\n","text = \"This is a sample sentence for whitespace tokenization.\"\n","tokens = text.split()\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irZbnE_8D91m","executionInfo":{"status":"ok","timestamp":1683108572015,"user_tz":-330,"elapsed":12,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"007119c2-5cf4-48f9-f50f-8327f4b04da9"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', 'for', 'whitespace', 'tokenization.']\n"]}]},{"cell_type":"code","source":["#Punctuation-Based Tokenization\n","text = \"This is a sample sentence for punctuation-based tokenization.\"\n","tokens = word_tokenize(text)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qukjRW2aD94M","executionInfo":{"status":"ok","timestamp":1683108572015,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"38e6d790-bd1b-4684-ae10-2439209f9411"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', 'for', 'punctuation-based', 'tokenization', '.']\n"]}]},{"cell_type":"code","source":["#Treebank Tokenization\n","tokenizer = TreebankWordTokenizer()\n","tokens = tokenizer.tokenize(text)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oUCftGUD97B","executionInfo":{"status":"ok","timestamp":1683108572016,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"8a2b79a3-40e9-400a-cdbc-01e9b2d9af25"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', 'for', 'punctuation-based', 'tokenization', '.']\n"]}]},{"cell_type":"code","source":["#Tweet Tokenization\n","tokenizer = TweetTokenizer()\n","tokens = tokenizer.tokenize(text)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaXRlDe3EXn6","executionInfo":{"status":"ok","timestamp":1683108572016,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"264e1f23-fc98-4813-dcbd-657cd134d150"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', 'for', 'punctuation-based', 'tokenization', '.']\n"]}]},{"cell_type":"code","source":["#MWE Tokenization (multi-word expressions)\n","tokenizer = MWETokenizer([('hot', 'dog'), ('New', 'York', 'City')])\n","tokens = tokenizer.tokenize(word_tokenize(text))\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WG24PLHEXqr","executionInfo":{"status":"ok","timestamp":1683108572016,"user_tz":-330,"elapsed":8,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"9e282b65-45d3-4f98-89ad-9f297691dc51"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', 'for', 'punctuation-based', 'tokenization', '.']\n"]}]},{"cell_type":"markdown","source":["##Stemming"],"metadata":{"id":"P95itYNOF9rj"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"],"metadata":{"id":"0rf2niWVEXtQ","executionInfo":{"status":"ok","timestamp":1683108572016,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Porter Stemming\n","porter_stemmer = PorterStemmer()\n","stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n","print(\"Porter Stemmed words:\", stemmed_words)"],"metadata":{"id":"T2NeD1DDEXv1","executionInfo":{"status":"ok","timestamp":1683108572016,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f58270bb-e07b-4c8d-8d0f-7b3ec9ad1d11"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Porter Stemmed words: ['thi', 'is', 'a', 'sampl', 'sentenc', 'for', 'punctuation-bas', 'token', '.']\n"]}]},{"cell_type":"code","source":["# Snowball Stemming\n","snowball_stemmer = SnowballStemmer('english')\n","snowball_stemmed_words = [snowball_stemmer.stem(word) for word in tokens]\n","print(\"Snowball Stemmed words:\", snowball_stemmed_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5mLgM-uFXFY","executionInfo":{"status":"ok","timestamp":1683108572017,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"6000c7b2-fb99-47d2-ec28-f7cf27b7da7a"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Snowball Stemmed words: ['this', 'is', 'a', 'sampl', 'sentenc', 'for', 'punctuation-bas', 'token', '.']\n"]}]},{"cell_type":"markdown","source":["##Lemmatization"],"metadata":{"id":"qAlIp6svF6LA"}},{"cell_type":"code","source":["# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n","print(\"Lemmatized words:\", lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WH8RUOFYFXM-","executionInfo":{"status":"ok","timestamp":1683108573920,"user_tz":-330,"elapsed":1909,"user":{"displayName":"Vrushaket Chaudhari","userId":"14684331638122896955"}},"outputId":"9d09cc74-7dda-4eb7-8856-357f6b531e5e"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemmatized words: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'punctuation-based', 'tokenization', '.']\n"]}]}]}